{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3b4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import numpy as np \n",
    "import torchvision.models as models \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from utils import * \n",
    "from model import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc43f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth Dataset \n",
    "data = np.load('./CompressedDatasets/cifar/dataset.npz')\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a834070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reconstruct_img(x, sigma, Vt) -> torch.Tensor: \n",
    "#     x = x @ (torch.diag(sigma) @  Vt)\n",
    "#     x = x.reshape(32, 32, 3)\n",
    "#     plt.imshow(x)\n",
    "#     x = x.permute(2, 0, 1)\n",
    "#     x = torch.clip(x, 0, 1)\n",
    "#     return x # [N, 3, 32, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cd2856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U shape: torch.Size([50000, 3072]), sigma shape: torch.Size([3072]), V shape: torch.Size([3072, 3072])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x3067a3b50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "U, sigma, V = prepare_dataset(3072)\n",
    "print(f\"U shape: {U.shape}, sigma shape: {sigma.shape}, V shape: {V.shape}\")\n",
    "\n",
    "reconstructed_img = reconstruct_img(U[1], sigma, V)\n",
    "\n",
    "reconstructed_img = reconstructed_img.flatten()\n",
    "U_test = (reconstructed_img @ torch.inverse(V)) @ torch.inverse(torch.diag(sigma))\n",
    "new_img = U_test @ torch.diag(sigma) @ V\n",
    "new_img = new_img.reshape(3, 32, 32)\n",
    "new_img = new_img.permute(1, 2, 0)\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dbf5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, test_loader): \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_test_tensor, y_test_tensor in test_loader:\n",
    "            y_test_tensor = y_test_tensor.flatten()\n",
    "            outputs = model(X_test_tensor)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == y_test_tensor).sum().item()  # Accumulate correct predictions\n",
    "            total += y_test_tensor.size(0)  # Accumulate total samples\n",
    "\n",
    "\n",
    "    accuracy = 100. * correct / total  # Calculate overall accuracy\n",
    "    # print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d638566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, optimizer, epochs, criterion, experiment_name = None): \n",
    "    train_accuracy = [] \n",
    "    val_accuracy = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0 \n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        val_acc = validate(model, test_loader)\n",
    "        val_accuracy.append(val_acc)\n",
    "\n",
    "        train_accuracy.append(correct / total)\n",
    "        # print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}, Accuracy: {correct / total:.2f}%\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}, Validation Accuracy: {val_acc}%\")\n",
    "\n",
    "    # Save the model\n",
    "    if experiment_name is not None:\n",
    "        torch.save(model.state_dict(), f\"./results/cifar-model/{experiment_name}.pth\")\n",
    "        print(f\"Model saved as {experiment_name}.pth\")\n",
    "    else: \n",
    "        torch.save(model.state_dict(), f\"./results/cifar-model/{experiment_name}.pth\")\n",
    "        print(f\"Model saved as {experiment_name}.pth\")\n",
    "\n",
    "    return max(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cab0d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yp/mbg8sv_n2cz_7gw8tbfw1tvm0000gn/T/ipykernel_4838/4109688549.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(U, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
      " 10%|█         | 1/10 [00:57<08:37, 57.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4672, Validation Accuracy: 55.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:55<07:40, 57.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.1157, Validation Accuracy: 63.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:53<06:44, 57.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.9633, Validation Accuracy: 66.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:51<05:47, 57.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.8632, Validation Accuracy: 68.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [04:48<04:49, 57.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.7840, Validation Accuracy: 70.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [05:56<04:03, 60.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.7105, Validation Accuracy: 70.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [07:05<03:10, 63.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.6456, Validation Accuracy: 71.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [08:13<02:10, 65.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.5891, Validation Accuracy: 70.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [09:22<01:06, 66.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.5288, Validation Accuracy: 71.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [10:30<00:00, 63.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.4697, Validation Accuracy: 71.2%\n",
      "Model saved as model_200.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 1/10 [01:07<10:11, 67.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4817, Validation Accuracy: 53.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:15<09:02, 67.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.1238, Validation Accuracy: 61.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:23<07:54, 67.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.9659, Validation Accuracy: 65.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [04:30<06:45, 67.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.8523, Validation Accuracy: 68.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [05:38<05:38, 67.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.7720, Validation Accuracy: 69.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [05:44<05:44, 68.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     30\u001b[39m experiment_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m accuracies.append(acc)\n\u001b[32m     34\u001b[39m singular_values.append(k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, test_loader, optimizer, epochs, criterion, experiment_name)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     13\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     labels = labels.squeeze()\n\u001b[32m     16\u001b[39m     loss = criterion(outputs, labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/image-distillation/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/image-distillation/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cornellSP2025/cs6386-main/distilled-image-dataset/model.py:33\u001b[39m, in \u001b[36mSimpleCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Apply SVD transformation\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x.shape) == \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreconstruct_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.conv1(x))\n\u001b[32m     36\u001b[39m x = F.max_pool2d(x, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/cornellSP2025/cs6386-main/distilled-image-dataset/model.py:22\u001b[39m, in \u001b[36mSimpleCNN.reconstruct_img\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreconstruct_img\u001b[39m(\u001b[38;5;28mself\u001b[39m, x) -> torch.Tensor: \n\u001b[32m     21\u001b[39m     x = x @ (torch.diag(\u001b[38;5;28mself\u001b[39m.sigma) @  \u001b[38;5;28mself\u001b[39m.Vt)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     24\u001b[39m     x = torch.clip(x, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data = np.load('./CompressedDatasets/cifar/dataset.npz')\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "singular_values = [] \n",
    "accuracies = [] \n",
    "\n",
    "for k in range(200, 3072, 200):  \n",
    "    U, sigma, V = prepare_dataset(k)\n",
    "    \n",
    "    # Define training parameters\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SimpleCNN(sigma, V).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(torch.tensor(U, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test, torch.tensor(y_test, dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Train the model\n",
    "    experiment_name = f\"model_{k}\"\n",
    "    acc = train(model, train_loader, test_loader, optimizer, epochs, criterion, experiment_name)\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    singular_values.append(k)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5002b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c8cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-distillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
